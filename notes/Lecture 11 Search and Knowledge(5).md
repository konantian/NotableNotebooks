---
title: Lecture 11 Search and Knowledge(5)
tags: [Notebooks/Cmput 496]
created: '2019-03-23T03:59:23.276Z'
modified: '2019-04-11T02:00:28.042Z'
---

# Lecture 11 -- Search and Knowledge(5)
## Knowledge in heuristic search
**Knowledge using in Heuristic Search**
  * Evaluate states
  * Evaluate action

**Interpretations of knowledge**
  * Probabilities(æ¦‚çŽ‡æ›´å¤§çš„moveä¼šæ›´å®¹æ˜“è¢«é€‰åˆ°)
  * Preferences(ä¼˜å…ˆçº§æ›´é«˜çš„moveä¼šæ›´å®¹æ˜“è¢«é€‰åˆ°)
  * Ordering
  * Ranking

**Representing knowledge**
  * Rules
  * Patterns
  * Features
  * Neural nets

**search techniques**
  * Knowledge free search
    * Blind search(å®Œå…¨éšæœºæœç´¢)
    * Random sampling
    * Boolean negamax or alphabeta
  * "Black box" heuristic evaluation function
    * Depth-limited alphabeta search(å·²çŸ¥good movesä¸ä¼šè¶…è¿‡æŸä¸ªæ·±åº¦)
    * Move ordering heuristic(å°†æ›´å¥½çš„moveä¼˜å…ˆæœç´¢)
    * State evaluation(stateè¯„ä¼°å‡½æ•°ï¼Œç”¨äºŽå†³å®šå“ªä¸ªstateä»·å€¼æ›´é«˜)
    * Move evaluation(moveè¯„ä¼°å‡½æ•°ï¼Œç”¨äºŽå†³å®šå“ªä¸ªmoveå¯ä»¥èµ°å‘æ›´å¥½çš„state)
    * Time control
    * Search depth control
    * DAG vs tree(å‡å°‘é‡å¤çš„state)
    * State representation(é€šè¿‡ç”¨1ç»´arrayæ¥ä»£è¡¨æ£‹ç›˜å¯ä»¥æå‡æœç´¢é€Ÿåº¦)

## Evaluation
**State Evaluation**
  * Mapping from(full) state to one number
  * Measure how good is that state
  * Also called "Evaluation function" and "position Evaluation"
  * Interpretation
    * Number: Represent the value of this state,used for **relative ranking** of state
    * Probability: Represent the probability that this state is the best state
  * Exact evaluation for leaf nodes and heuristic evaluation of non-terminal states(é€šè¿‡è®¡ç®—leaf nodeçš„ä»·å€¼å›žä¼ ç»™ä¸Šé¢ðŸ‘†çš„nodeæ¥è®¡ç®—åˆ«çš„nodeçš„ä»·å€¼)


**Move Evaluation**
  * Mapping from move(Action) to number
  * Measure how good is that move
  * Use as a filter to prune bad moves
  * Also called "Action value" "Move value" and "Q-value in machine learning" 
  * Interpretation
    * Number: Represent the value of this move,used for **relative ranking** of moves(æ•°å­—æœ¬èº«ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯ç›¸å¯¹çš„ä¼˜å…ˆé¡ºåº)
    * Probability: Represent the probability that this move is the best move

**Evaluation Interpretation**
  * Similar evaluation values for similar states
  * All states with the same evaluation are "equally good"
  * Same probability of winning for equally good states
  * Higher evaluation = higher probability of winning

**Winning Probability interpretation**
  * Winning probability is the minimax score
  * Pefect player would know the certainty of the winning probability
  * Can use machine learning estimates of win probabilities

**Relation between state evaluation and move evaluation**
  * If we have state evaluation, it is easy to get move evaluation
    * Do 1 ply search
    * Slow with large branching factor
  * If we have move evaluation, it is hard to get state evaluation
    * Slow and noisy result

## Evaluation Knowledge
**Acquiring Knowledge**
  * Machine learning
  * Local goal-directed search
  * Hancoded rules

**Representing knowledge**
  * Handcoded rules
  * Simple features
    * Each feature is a boolean state about a state or a move
    * Move feature vector: $(0,0,1,...,1,1,0,...,1,0,...0,0,...)$
  * Pattern databases
  * Neural nets

**Benson's Algorithm**
> Benson's algorithm finds stones and territories that are unconditionally alive



